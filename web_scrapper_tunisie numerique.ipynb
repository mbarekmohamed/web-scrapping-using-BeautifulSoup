{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping using Beautifulsoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting all the the URLs from pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrapping page num  1\n"
     ]
    }
   ],
   "source": [
    "relative_urls=[]\n",
    "for i in range (1,16):\n",
    "    url='https://www.tunisienumerique.com/actualite-tunisie/tunisie/page'+str(i)+'/'\n",
    "    print('scrapping page num ',i)\n",
    "    response=requests.get(url)\n",
    "    response.status_code\n",
    "\n",
    "    html=response.content\n",
    "\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "\n",
    "    #Creating links and fetching articles\n",
    "    links = []\n",
    "\n",
    "\n",
    "    x=soup.find_all('li',class_=\"infinite-post\")\n",
    "\n",
    "\n",
    "    aa = BeautifulSoup(str(x), \"html.parser\")\n",
    "\n",
    "    links = aa.find_all('a')\n",
    "\n",
    "    relative_url = [link.get('href') for link in links]\n",
    "    for url in relative_url:\n",
    "        relative_urls.append(url)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting all the text from the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []\n",
    "titles =  []\n",
    "types=[]\n",
    "for link in relative_urls :\n",
    "        result = requests.get(link)\n",
    "        text=result.text\n",
    "        soup = BeautifulSoup(text,'html.parser')\n",
    "        \n",
    "        ar = soup.find_all('div', id=\"content-main\",class_=\"left relative\")\n",
    "        art = BeautifulSoup(str(ar), \"html.parser\")\n",
    "        article = art.find_all('p')\n",
    "        article_txt = [t.text for t in article]\n",
    "        articles.append(article_txt)\n",
    "        \n",
    "        title = soup.find_all('h1', class_='post-title entry-title left')\n",
    "        title_txt = [t.text for t in title]\n",
    "        titles.append(title_txt)\n",
    "        \n",
    "        ty=soup.find_all('span', class_=\"post-head-cat\")  \n",
    "        type_txt = [t.text for t in ty]\n",
    "        types.append(type_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_str=[\"\".join(text) for text in titles]\n",
    "articles_str=[\"\".join(text) for text in articles]\n",
    "type_str=[\"\".join(text) for text in types]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(relative_urls, titles_str , articles_str,type_str)), \n",
    "                columns =['link','titles', 'article','type'])\n",
    "df[\"type\"].replace(to_replace='\\xa0',value=np.nan,inplace=True)\n",
    "df.to_csv('tunisie_numerique.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Web Scraping using Beautifulsoup\n",
    "\n",
    "### Importing the packages\n",
    "\n",
    "import bs4 as bs\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Extracting all the the URLs from pages \n",
    "\n",
    "relative_urls=[]\n",
    "for i in range (1,16):\n",
    "    url='https://www.tunisienumerique.com/actualite-tunisie/tunisie/page'+str(i)+'/'\n",
    "    print('scrapping page num ',i)\n",
    "    response=requests.get(url)\n",
    "    response.status_code\n",
    "\n",
    "    html=response.content\n",
    "\n",
    "    soup=BeautifulSoup(html,'html.parser')\n",
    "\n",
    "    #Creating links and fetching articles\n",
    "    links = []\n",
    "\n",
    "\n",
    "    x=soup.find_all('li',class_=\"infinite-post\")\n",
    "\n",
    "\n",
    "    aa = BeautifulSoup(str(x), \"html.parser\")\n",
    "\n",
    "    links = aa.find_all('a')\n",
    "\n",
    "    relative_url = [link.get('href') for link in links]\n",
    "    for url in relative_url:\n",
    "        relative_urls.append(url)\n",
    "    \n",
    "\n",
    "\n",
    "# Extracting all the text from the URLs\n",
    "\n",
    "articles = []\n",
    "titles =  []\n",
    "types=[]\n",
    "for link in relative_urls :\n",
    "        result = requests.get(link)\n",
    "        text=result.text\n",
    "        soup = BeautifulSoup(text,'html.parser')\n",
    "        \n",
    "        ar = soup.find_all('div', id=\"content-main\",class_=\"left relative\")\n",
    "        art = BeautifulSoup(str(ar), \"html.parser\")\n",
    "        article = art.find_all('p')\n",
    "        article_txt = [t.text for t in article]\n",
    "        articles.append(article_txt)\n",
    "        \n",
    "        title = soup.find_all('h1', class_='post-title entry-title left')\n",
    "        title_txt = [t.text for t in title]\n",
    "        titles.append(title_txt)\n",
    "        \n",
    "        ty=soup.find_all('span', class_=\"post-head-cat\")  \n",
    "        type_txt = [t.text for t in ty]\n",
    "        types.append(type_txt)\n",
    "\n",
    "titles_str=[\"\".join(text) for text in titles]\n",
    "articles_str=[\"\".join(text) for text in articles]\n",
    "type_str=[\"\".join(text) for text in types]\n",
    "\n",
    "# creating csv file\n",
    "\n",
    "df = pd.DataFrame(list(zip(relative_urls, titles_str , articles_str,type_str)), \n",
    "                columns =['link','titles', 'article','type'])\n",
    "df[\"type\"].replace(to_replace='\\xa0',value=np.nan,inplace=True)\n",
    "df.to_csv('tunisie_numerique.csv',index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
